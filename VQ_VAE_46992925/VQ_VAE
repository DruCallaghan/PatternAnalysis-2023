'''
VQ-VAE
Model as implemented by
https://www.youtube.com/watch?v=1ZHzAOutcnw
'''
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
from tqdm import tqdm
from PIL import Image
import torch.utils.data
from torchvision import datasets, transforms, utils
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Torch version ", torch.__version__)


# ------------------------------------------------
# Data Loader

path = "C:/Users/61423/COMP3710/data/keras_png_slices_data/"

def load_data_from_folder(name):
    data = []
    for filename in os.listdir(path+name):
        
        image_path = os.path.join(path+name, filename)
        image = Image.open(image_path)
        image = np.array(image)  
        data.append(image)
        
    return np.array(data)

print("> Loading Test data")

########## TODO THIS IS NOT SEGMENTATION!! SPLIT TRAINING INTO TRAIN AND TEST!!!

train_data = torch.from_numpy(load_data_from_folder("keras_png_slices_train/")).to(device)
test_data = torch.from_numpy(load_data_from_folder("keras_png_slices_test/")).to(device)

train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)


print("> Test Data Finsihed Loading")
print("The shape of the data is: ", train_data.shape)



# ------------------------------------------------
# Model

class VQVAE(nn.Module):
    def __init__(self, ):
        super(VQVAE, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(4),
            nn.ReLU(),
        )
        
        self.pre_quant_conv = nn.Conv2d(4, 2, kernel_size=1)        # TODO FC layer??
        self.embedding = nn.Embedding(num_embeddings=3, embedding_dim=2)
        self.post_quant_conv = nn.Conv2d(2, 4, kernel_size=1)
        
        # Commitment loss beta
        self.beta = 0.2
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(4, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh(),
        )
            
    def forward(self, x):
        # B, C, H, W
        encoded_output = self.encoder(x)
        quant_input = self.pre_quant_conv(encoded_output)
        
        # Quantisation
        B, C, H, W = quant_input.shape
        quant_input = quant_input.permute(0, 2, 3, 1)
        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1)))
        
        # Compute pairwise distances
        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))
        
        # Find index of nearest embedding
        min_encoding_indices = torch.argmin(dist, dim=-1)
        
        # Select the embedding weights
        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))
        
        quant_input = quant_input.reshape((-1, quant_input.size(-1)))
        
        # Compute losses
        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2)
        codebook_loss = torch.mean((quant_out - quant_input.detach())**2)
        
        
        # Straight through gradient estimator
        quant_out = quant_input + (quant_out - quant_input).detach()        # Detach ~ ignored for back-prop
        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)
        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))
        
        # Decoding
        decoder_input = self.post_quant_conv(quant_out)
        output = self.decoder(decoder_input)
        
        # Reconstruction Loss, and find the total loss
        reconstruction_loss = F.mse_loss(x, output)
        total_losses = reconstruction_loss + codebook_loss + self.beta*commitment_loss
        
        return output, total_losses
    
    
# ------------------------------------------------
# Training

########################## TODO THERE IS NO RECONSTRUCTION LOSS!!

# Hyperparams
learning_rate = 1.e-3
num_epochs = 3


model = VQVAE().to(device)
print(model)

optimiser = torch.optim.Adam(model.parameters(), learning_rate)

for epoch_num, epoch in enumerate(tqdm(range(num_epochs))):
    model.train()
    for train_batch in tqdm(train_dataloader):
        images, labels = train_batch
        images = images.to(device)
        
        output, total_losses = model(images)

        optimiser.zero_grad()       # Reset gradients to zero for back-prop
        total_losses.backward()     # Calculate grad
        optimiser.step()
        
    # Evaluate
    model.eval()
    
    for test_batch in tqdm(test_dataloader):
        images, labels = test_batch
        images = images.to(device)
        
        with torch.no_grad():
            output, total_losses = model(images)
            
    print("Epoch {} of {}. Total Loss: {}".format(epoch_num, num_epochs, total_losses))