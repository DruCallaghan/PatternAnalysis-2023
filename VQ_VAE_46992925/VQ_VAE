'''
VQ-VAE
Model as implemented by
https://www.youtube.com/watch?v=1ZHzAOutcnw
'''
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np  
from tqdm import tqdm
from PIL import Image
import torch.utils.data
from torchvision import datasets, transforms, utils
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Torch version ", torch.__version__)


# ------------------------------------------------
# Data Loader

path = "C:/Users/61423/COMP3710/data/keras_png_slices_data/"
#path = "//puffball.labs.eait.uq.edu.au/s4699292/Documents/2023 Sem2/Comp3710/keras_png_slices_data/keras_png_slices_data/"

def load_data_from_folder(name):
    data = []
    i = 0
    
    for filename in tqdm([f for f in os.listdir(path+name) if f.lower().endswith('.png')]):            # tqdm adds loading bar!
        
        image_path = os.path.join(path+name, filename)
        image = Image.open(image_path).convert('L')         # Convert to grayscale (single channel)
        image = np.array(image)
        
        # Add channel
        # C, H, W
        image = np.expand_dims(image, axis=0)

        data.append(image)

        if i == 25:
            return np.array(data)
        i += 1

    return np.array(data)

# Loading
# B, C, H, W (Numpy array)
print("> Loading Training data")
train_data = (load_data_from_folder("keras_png_slices_train/"))
print("> Loading Test data")
test_data = (load_data_from_folder("keras_png_slices_test/"))

print("The shape of the (training) data is: ", train_data.shape)
print("The shape of the (testing) data is: ", test_data.shape)

# Transforms and tensor
mean = np.mean(train_data)
std = np.std(train_data)

transform = transforms.Compose([
                                transforms.ToTensor(),
                                transforms.Normalize(mean=0.13242, std=0.18826)
])

train_data = torch.stack([transform(item) for item in train_data]).permute(0, 2, 3, 1)
test_data = torch.stack([transform(item) for item in test_data]).permute(0, 2, 3, 1)

print("The shape of the (training) data is: ", train_data.shape)
print("The shape of the (testing) data is: ", test_data.shape)

# DataLoaders
# B, C, H, W
train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)

#plt.imshow(train_data[0][0])
#plt.title("First Training image (Normalised)")
#plt.gray()
#plt.show()

print("> Data Loading Finished")

# ------------------------------------------------
# Model



class Encoder(nn.Module):
    def __init__(self, ):
        super(Encoder, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        )
        
    def forward(self, x):
        out = self.encoder(x)
        return out


class Quantiser(nn.Module):
    def __init__(self, num_embeddings, embedding_dim) -> None:
        super(Quantiser, self).__init__()
        
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.beta = 0.2
        
        self.embedding = self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)


    def get_encoding_indices(self, quant_input):
        # Flatten
        quant_input = quant_input.permute(0, 2, 3, 1)
        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1)))
    
        # Compute pairwise distances
        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))
        
        # Find index of nearest embedding
        encoding_indices = torch.argmin(dist, dim=-1)       # in form B, W*H
        return encoding_indices
    
    def output_from_indices(self, indices, output_shape):
        quant_out = torch.index_select(self.embedding.weight, 0, indices.view(-1))
        quant_out = quant_out.reshape(output_shape).permute(0, 3, 1, 2)
        return quant_out
    
    def forward(self, quant_input):
        
        # Get the encoding indices
        encoding_indices = self.get_encoding_indices(quant_input)

        quant_out = self.output_from_indices(encoding_indices, quant_input.shape)

        print(quant_out.shape, quant_input.shape)
        
        # Compute losses
        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2)             # TODO change to MSE
        codebook_loss = torch.mean((quant_out - quant_input.detach())**2)
        loss = codebook_loss + self.beta*commitment_loss
        
        # Straight through gradient estimator
        quant_out = quant_input + (quant_out - quant_input).detach()        # Detach ~ ignored for back-prop

        # Reshape encoding indices to 'B, H, W'
        # TODO CURRENTLY MEANS NOTHING
        encoding_indices = encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))
        
        return quant_out, loss, encoding_indices


class Decoder(nn.Module):
    def __init__(self, ) -> None:
        super(Decoder, self).__init__()
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(1),
        )
        
    def forward(self, x):
        out = self.decoder(x)
        return out
            



class VQVAE(nn.Module):
    def __init__(self, num_embeddings, embedding_dim):
        super(VQVAE, self).__init__()
        
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        
        self.encoder = Encoder()
        self.quantiser = Quantiser(num_embeddings=num_embeddings, embedding_dim=embedding_dim)
        self.decoder = Decoder()
        
            
    def forward(self, x):
        # B, C, H, W
        quant_input = self.encoder(x)
        quant_out, quant_loss, encoding_indices = self.quantiser(quant_input)
        output = self.decoder(quant_out)
        
        # Reconstruction Loss, and find the total loss
        reconstruction_loss = F.mse_loss(x, output)
        total_loss = quant_loss + reconstruction_loss
        
        return output, total_loss, encoding_indices
    
    @torch.no_grad()
    def img_from_indices(self, indices):
        quant_out = self.quantiser.output_from_indices(indices, (26, 1, 32, 32))
        return self.decoder(quant_out)
    
# ------------------------------------------------
# Training

losses = []     # for visualisation

# Hyperparams
learning_rate = 1.e-3
num_epochs = 3

num_embeddings = 256
embedding_dim = 32



model = VQVAE(num_embeddings=num_embeddings, embedding_dim=embedding_dim).to(device)
print(model)

optimiser = torch.optim.Adam(model.parameters(), learning_rate)


for epoch_num, epoch in enumerate(range(num_epochs)):
    model.train()
    for train_batch in tqdm(train_dataloader):
        images = train_batch
        images = images.to(device, dtype=torch.float32)
        
        output, total_losses, _ = model(images)

        optimiser.zero_grad()       # Reset gradients to zero for back-prop (not cumulative)
        total_losses.backward()     # Calculate grad
        optimiser.step()            # Adjust weights
        
   
    # Evaluate
    model.eval()
    
    for test_batch in (test_dataloader):
        images = test_batch

        images = images.to(device, dtype=torch.float32)         # (Set as float to ensure weights input are the same type)
        
        with torch.no_grad():
            output, total_losses, _ = model(images)

            
    print("Epoch {} of {}. Total Loss: {}".format(epoch_num, num_epochs, total_losses))
    
    losses.append(total_losses.cpu())     # To graph losses (TODO still in tensors)

# -------------------------------------------------
# Visualise


def plot_results(num_images):

    input_imgs = test_data[0:num_images]
    input_imgs = input_imgs.to(device, dtype=torch.float32)

    # DEBUGGING
    print("Shape of the input img is: ", input_imgs.shape)

    with torch.no_grad():  # Ensure no gradient calculation
        output_imgs, _, encoding_indices = model(input_imgs)
        
        
    #Debugging
    print("Shape of the output img is: ", output_imgs.shape)
    print("Enc indices shape is: ", encoding_indices.shape)


    fig, ax = plt.subplots(num_images, 3)
    plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0)

    ax[0, 0].set_title("Inputs")
    ax[0, 1].set_title("CodeBook Indices")
    ax[0, 2].set_title("Reconstruction")

    for i in range(num_images):
        for j in range(3):
            ax[i, j].axis('off')
        ax[i, 0].imshow(input_imgs[i][0].cpu().numpy(), cmap='gray')
        ax[i, 1].imshow(encoding_indices[i].cpu().numpy())
        ax[i, 2].imshow(output_imgs[i][0].cpu().numpy(), cmap='gray')
    
    plt.show()

    plt.plot(losses)
    plt.title("Losses")
    plt.xlabel("Num Epochs")
    plt.ylabel("Loss")
    plt.show()

plot_results(2)



# ------------- Pixel CNN
# Define the PixelConvLayer class in PyTorch

class MaskedConvolution(nn.Module):

    def __init__(self, in_channels, out_channels, mask, dilation=1):
        
        super(MaskedConvolution, self).__init__()
        kernel_size = (mask.shape[0], mask.shape[1])
        padding = ([dilation*(kernel_size[i] - 1) // 2 for i in range(2)])
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)

        # Mask as buffer (must be moved with devices)
        self.register_buffer('mask', mask[None,None])

    def forward(self, x):
        self.conv.weight.data *= self.mask # Set all following weights to 0
        return self.conv(x)


class VerticalConv(MaskedConvolution):
    # Masks all pixels below
    def __init__(self, in_channels, out_channels, kernel_size=3, mask_center=False, dilation=1):
        mask = torch.ones(kernel_size, kernel_size)
        mask[kernel_size//2+1:,:] = 0
        # For the first convolution, mask center row
        if mask_center:
            mask[kernel_size//2,:] = 0

        super().__init__(in_channels, out_channels, mask, dilation=dilation)

class HorizontalConv(MaskedConvolution):

    def __init__(self, in_channels, out_channels, kernel_size=3, mask_center=False, dilation=1):
        # Mask out all pixels on the left. (Note that kernel has a size of 1
        # in height because we only look at the pixel in the same row)
        mask = torch.ones(1,kernel_size)
        mask[0,kernel_size//2+1:] = 0

        # For first convolution, mask center pixel
        if mask_center:
            mask[0,kernel_size//2] = 0

        super().__init__(in_channels, out_channels, mask, dilation=dilation)
        
        
class GatedMaskedConv(nn.Module):

    def __init__(self, in_channels, dilation=1):

        super(GatedMaskedConv, self).__init__()
        self.conv_vert = VerticalConv(in_channels, out_channels=2*in_channels, dilation=dilation)
        self.conv_horiz = HorizontalConv(in_channels, out_channels=2*in_channels, dilation=dilation)
        self.conv_vert_to_horiz = nn.Conv2d(2*in_channels, 2*in_channels, kernel_size=1, padding=0)
        self.conv_horiz_1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)

    def forward(self, v_stack, h_stack):
        # Vertical stack (left)
        v_stack_feat = self.conv_vert(v_stack)
        v_val, v_gate = v_stack_feat.chunk(2, dim=1)
        v_stack_out = torch.tanh(v_val) * torch.sigmoid(v_gate)

        # Horizontal stack (right)
        h_stack_feat = self.conv_horiz(h_stack)
        h_stack_feat = h_stack_feat + self.conv_vert_to_horiz(v_stack_feat)
        h_val, h_gate = h_stack_feat.chunk(2, dim=1)
        h_stack_feat = torch.tanh(h_val) * torch.sigmoid(h_gate)
        h_stack_out = self.conv_horiz_1x1(h_stack_feat)
        h_stack_out = h_stack_out + h_stack

        return v_stack_out, h_stack_out
    
class PixelCNN(nn.Module):
    
    def __init__(self, in_channels, hidden_channels):
        super().__init__()

        # Initial convolutions skipping the center pixel
        self.conv_vstack = VerticalConv(in_channels, hidden_channels, mask_center=True)
        self.conv_hstack = HorizontalConv(in_channels, hidden_channels, mask_center=True)
        # Convolution block of PixelCNN. Uses dilation instead of downscaling
        self.conv_layers = nn.ModuleList([
            GatedMaskedConv(hidden_channels),
            GatedMaskedConv(hidden_channels, dilation=2),
            GatedMaskedConv(hidden_channels),
            GatedMaskedConv(hidden_channels, dilation=4),
            GatedMaskedConv(hidden_channels),
            GatedMaskedConv(hidden_channels, dilation=2),
            GatedMaskedConv(hidden_channels)
        ])
        # Output classification convolution (1x1)
        # The output channels should be in_channels*number of embeddings to learn continuous space and calc. CrossEntropyLoss
        self.conv_out = nn.Conv2d(hidden_channels, in_channels*num_embeddings, kernel_size=1, padding=0)
        
        
    def forward(self, x):
        # Scale input from 0 to 255 to -1 to 1
        x = (x.float() / 255.0) * 2 - 1

        # Initial convolutions
        v_stack = self.conv_vstack(x)
        h_stack = self.conv_hstack(x)
        # Gated Convolutions
        for layer in self.conv_layers:
            v_stack, h_stack = layer(v_stack, h_stack)
        # 1x1 classification convolution
        # Apply ELU (exponential activation function) before 1x1 convolution for non-linearity on residual connection
        out = self.conv_out(F.elu(h_stack))

        # Output dimensions: [Batch, Classes, Channels, Height, Width] (classes = num_embeddings)
        out = out.reshape(out.shape[0], num_embeddings, out.shape[1]//256, out.shape[2], out.shape[3])
        return out

    """Indices shape should be in form B C H W
    Pixels to fill should be marked with -1"""
    @torch.no_grad()
    def sample(self, ind_shape, ind=None):
        # Create tensor of indices (all -1)
        if ind is None:
            ind = torch.zeros(ind_shape, dtype=torch.long).to(device) - 1
        # Generation loop (iterating through pixels across channels)
        for h in range(ind_shape[2]):                   # Heights
            for w in range(ind_shape[3]):               # Widths
                for c in range(ind_shape[1]):           # Channels
                    # Skip if not to be filled (-1)
                    if (ind[:,c,h,w] != -1).all().item():
                        continue
                    # Only have to input upper half of ind (rest are masked anyway)
                    pred = self.forward(ind[:,:,:h+1,:])
                    probs = F.softmax(pred[:,:,c,h,w], dim=-1)
                    ind[:,c,h,w] = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)
        return ind



cnn_model = PixelCNN(in_channels=1, hidden_channels=128)
optimiser = torch.optim.Adam(cnn_model.parameters(), learning_rate)

# For getting codebook indices
encoder = model.__getattr__("encoder")
quantiser = model.__getattr__("quantiser")
decoder = model.__getattr__("decoder")

for epoch in range(num_epochs):
    
    cnn_model.train()
    
    for train_batch in train_dataloader:
        
        # Get the quantised outputs
        with torch.no_grad():
            encoder_output = encoder(train_batch)
            _, _, indices = quantiser(encoder_output)
            indices = indices.reshape(indices.size(0), 1, indices.size(1), indices.size(2)).to(device)
                        
        output = cnn_model(indices)
        
        # Compute loss
        nll = F.cross_entropy(output, indices, reduction='none')        # Negative log-likelihood
        bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))               # Bits per dimension
        loss = bpd.mean()

        optimiser.zero_grad()       # Reset gradients to zero for back-prop (not cumulative)
        loss.backward()             # Calculate grad
        optimiser.step()            # Adjust weights
        
    cnn_model.eval()
    
    for test_batch in test_dataloader:
        # Get the quantised outputs
        with torch.no_grad():
            encoder_output = encoder(test_batch)
            _, _, indices = quantiser(encoder_output)
            indices = indices.reshape(indices.size(0), 1, indices.size(1), indices.size(2)).to(device)
            
            
        output = cnn_model(indices)
        print("Indices is shape: ", indices.detach().cpu().numpy().shape)
        print("Output is shape: ", output.detach().cpu().numpy().shape)

        
        # Compute loss
        nll = F.cross_entropy(output, indices, reduction='none')        # Negative log-likelihood
        bpd = nll.mean(dim=[1,2,3]) * np.log2(np.exp(1))               # Bits per dimension
        loss = bpd.mean()
        
    print("Epoch {} of {}. Total Loss: {}".format(epoch_num, num_epochs, loss))
    
# Show one image
print(" > Showing Images")
print("Real indices shape: ", indices.detach().cpu().numpy().shape)
gen_indices = cnn_model.sample((1, 1, 16, 16))
print("Gen indices shape: ", gen_indices.detach().cpu().numpy().shape)

fig, ax = plt.subplots(2, 2)
ax[0, 0].set_title("Real Indices")
ax[0, 0].imshow(indices[0][0].long())
ax[0, 1].set_title("Real Decoded")
ax[0, 1].imshow(model.img_from_indices(indices)[0][0])



ax[1, 0].set_title("Generated Indices")
ax[1, 0].imshow(gen_indices[0][0])
ax[1, 1].set_title("Generated Image")
ax[1, 1].imshow(model.img_from_indices(gen_indices)[0][0])
plt.show()