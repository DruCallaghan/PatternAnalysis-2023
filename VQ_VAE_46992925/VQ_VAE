'''
VQ-VAE
Model as implemented by
https://www.youtube.com/watch?v=1ZHzAOutcnw
'''
import os
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
from tqdm import tqdm
from PIL import Image
import torch.utils.data
from torchvision import datasets, transforms, utils
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Torch version ", torch.__version__)


# ------------------------------------------------
# Data Loader

path = "C:/Users/61423/COMP3710/data/keras_png_slices_data/"

def load_data_from_folder(name):
    data = []
    for filename in os.listdir(path+name):
        
        image_path = os.path.join(path+name, filename)
        image = Image.open(image_path)
        image = np.array(image)  
        data.append(image)
        
    return np.array(data)

print("> Loading Test data")

train_data = torch.from_numpy(load_data_from_folder("keras_png_slices_test/")).to(device)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)

print("> Test Data Finsihed Loading")
print("The shape of the data is: ", train_data.shape)



# ------------------------------------------------
# Model

class VQVAE(nn.Module):
    def __init__(self, ):
        super(VQVAE, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(4),
            nn.ReLU(),
        )
        
        self.pre_quant_conv = nn.Conv2d(4, 2, kernel_size=1)        # TODO FC layer??
        self.embedding = nn.Embedding(num_embeddings=3, embedding_dim=2)
        self.post_quant_conv = nn.Conv2d(2, 4, kernel_size=1)
        
        # Commitment loss beta
        self.beta = 0.2
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(4, 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh(),
        )
            
    def forward(self, x):
        # B, C, H, W
        encoded_output = self.encoder(x)
        quant_input = self.pre_quant_conv(encoded_output)
        
        # Quantisation
        B, C, H, W = quant_input.shape
        quant_input = quant_input.permute(0, 2, 3, 1)
        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1)))
        
        # Compute pairwise distances
        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))
        
        # Find index of nearest embedding
        min_encoding_indices = torch.argmin(dist, dim=-1)
        
        # Select the embedding weights
        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))
        
        quant_input = quant_input.reshape((-1, quant_input.size(-1)))
        
        # Compute losses
        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2)
        codebook_loss = torch.mean((quant_out - quant_input.detach())**2)
        total_losses = codebook_loss + self.beta*commitment_loss
        
        # Straight through gradient estimator
        quant_out = quant_input + (quant_out - quant_input).detach()        # Detach ~ ignored for back-prop
        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)
        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))
    
    
        # Decoding
        decoder_input = self.post_quant_conv(quant_out)
        output = self.decoder(decoder_input)
        return output, total_losses
    
model = VQVAE()
print(model)